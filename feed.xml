<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shawnless.github.io/Personal//feed.xml" rel="self" type="application/atom+xml"/><link href="https://shawnless.github.io/Personal//" rel="alternate" type="text/html"/><updated>2025-02-13T06:40:08+00:00</updated><id>https://shawnless.github.io/Personal//feed.xml</id><title type="html">Shaolin Xie</title><subtitle>God help those who help themselves. </subtitle><entry><title type="html">Fundamental Math for Machine Learning</title><link href="https://shawnless.github.io/Personal//2025/01/27/Fundamental-Math-for-Machine-Learning.html" rel="alternate" type="text/html" title="Fundamental Math for Machine Learning"/><published>2025-01-27T00:00:00+00:00</published><updated>2025-01-27T00:00:00+00:00</updated><id>https://shawnless.github.io/Personal//2025/01/27/Fundamental-Math-for-Machine-Learning</id><content type="html" xml:base="https://shawnless.github.io/Personal//2025/01/27/Fundamental-Math-for-Machine-Learning.html"><![CDATA[<h2 id="linear-regression">Linear Regression</h2> <p>Linear Regression is an algorithm which predicts unknown value with existing data set. It models the factors and results as linear function, for example:</p> \[h_\theta (x)=\theta_0 + \theta_1x1 + \theta_2x2\] <p>where $x_1,x_2$ are the <strong>factors</strong> which affect the result $h_\theta (x)$. $x_1,x_2$ are also called <strong>features</strong> in deep learning. $\theta_0, \theta_1,\theta_2$ are the parameters or <strong>weights</strong> in deep learning, which are supposed to be fixed during the prediction or <strong>inference</strong>.</p> <p>If we let $x_0$=1, then we can write the above equation in a more general form as</p> \[h_\theta (x) =\sum_{i=1}^{n}\theta_ix_i=\mathbf{\theta}^T\mathbf{x}\] <p>where</p> \[\mathbf{\theta} = \begin{bmatrix} \theta_0 \\ \theta_1 \\ ... \\ \theta_n \\ \end{bmatrix} , \mathbf{x} = \begin{bmatrix} x_0 \\ x_1 \\ ... \\ x_n \\ \end{bmatrix}\] <p>Given an concrete example: we suppose the house price is highly related to 1. area, and 2. number of bedroom, how can we predict the price given its area and number of bedrooms?</p> <table> <thead> <tr> <th>Living area ($ft^2$)</th> <th>#bedrooms</th> <th>price (k)</th> </tr> </thead> <tbody> <tr> <td>2104</td> <td>3</td> <td>400</td> </tr> <tr> <td>1600</td> <td>3</td> <td>330</td> </tr> <tr> <td>2400</td> <td>3</td> <td>369</td> </tr> <tr> <td>1416</td> <td>2</td> <td>232</td> </tr> <tr> <td>3000</td> <td>4</td> <td>540</td> </tr> <tr> <td>…</td> <td>…</td> <td>…</td> </tr> </tbody> </table> <p>In this example: $x_1$ is the living area, $x_2$ is the number of bedrooms, <strong>y</strong> is the price of the house.</p> <p>The table about is called a <strong>training set</strong>, which will be used to training our model (that is the value of $\theta$s).</p> <p>The straight thought is to choose the hypnosis <strong>h(x)</strong> close to the training data <strong>y</strong>.</p> <p>This can be expressed with <strong>cost function</strong>. There can be many types of cost functions, the most popular one is least-squares cost function:</p> \[J(\theta) = \frac{1}{2}\sum_{i=1}^{n} (h_\theta(x^i) - y^i)^2\] <h3 id="gradient-descent-algorithm">Gradient Descent Algorithm</h3> <p>Gradient Descent Algorithm is used to find the value of $\theta$ so that the $J(\theta)$ is minimized, which can be described as following:</p> \[\theta_j := \theta_j - \alpha \frac{\delta J(\theta ) }{\delta \theta_j}\] <p>Here $\delta$ is called <strong>learning rate</strong>.</p> <p>Expend the $J(\theta)$ we can get the update equation:</p> \[\theta_j := \theta_j + \alpha \sum_{i=1}^{m} (y^i - h_\theta(x^i))x_j^i\] <p>where <strong>i</strong> is the index of data sets, <strong>j</strong> is the index of <strong>features</strong></p> <h4 id="batch-gradient-descent">batch gradient descent</h4> <p>In the above method, we look at every example in the entire training set on every step, and is called <strong>batch gradient descent</strong></p> <hr/>]]></content><author><name></name></author><category term="Math"/><category term="ML"/><summary type="html"><![CDATA[A learning notes of the basic math knowledge and conceptions for machine learning.]]></summary></entry></feed>